{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc806942",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc806942",
        "outputId": "eceb781a-6019-42af-fcff-3652ca1f2a23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import words\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('words')\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import string\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from node2vec import Node2Vec\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import wordsegment\n",
        "wordsegment.load()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "graph = nx.read_graphml(\"drive/MyDrive/web/database_formated_for_NetworkX.graphml\")\n",
        "graph = nx.DiGraph(graph)\n",
        "graph"
      ],
      "metadata": {
        "id": "3ulSTHi_DkPa"
      },
      "id": "3ulSTHi_DkPa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordsegment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPrS-QdA_Cx_",
        "outputId": "6bc30dee-55db-4635-f775-60cf2d3f2826"
      },
      "id": "EPrS-QdA_Cx_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wordsegment\n",
            "  Downloading wordsegment-1.3.1-py2.py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wordsegment\n",
            "Successfully installed wordsegment-1.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ff32da6",
      "metadata": {
        "id": "6ff32da6"
      },
      "source": [
        "# Preparation du dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "169f57f0",
      "metadata": {
        "id": "169f57f0"
      },
      "outputs": [],
      "source": [
        "#On sélectionne le contenu des tweets etles infos qui leur sont liées\n",
        "\n",
        "tweets = {n: graph.nodes[n]['text'] for n in graph.nodes if 'text' in graph.nodes[n]}\n",
        "priority = {n: graph.nodes[n]['annotation_postPriority'] for n in graph.nodes if 'annotation_postPriority' in graph.nodes[n]}\n",
        "Retweet = {n: graph.nodes[n]['retweet_count'] for n in graph.nodes if 'retweet_count' in graph.nodes[n]}\n",
        "Sensitivity = {n: graph.nodes[n]['possibly_sensitive'] for n in graph.nodes if 'possibly_sensitive' in graph.nodes[n]}\n",
        "Favori = {n: graph.nodes[n]['favorite_count'] for n in graph.nodes if 'favorite_count' in graph.nodes[n]}\n",
        "Tweet_df = pd.DataFrame({'noeud_tweet' : tweets.keys(), 'Contenu' : tweets.values(), 'Niveau': priority.values(),'Nb_retweet': Retweet.values(),\n",
        "                        'Sensitivité': Sensitivity.values(), 'Favori' : Favori.values()})\n",
        "Tweet_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05c0488f",
      "metadata": {
        "scrolled": true,
        "id": "05c0488f"
      },
      "outputs": [],
      "source": [
        "#On recupere les infos sur les utilisateurs\n",
        "\n",
        "Utilisateurs_nom = {n: graph.nodes[n]['name'] for n in graph.nodes if 'name' in graph.nodes[n]}\n",
        "Nom_secret = {n: graph.nodes[n]['screen_name'] for n in graph.nodes if 'screen_name' in graph.nodes[n]}\n",
        "Compte_verif = {n: graph.nodes[n]['isVerified'] for n in graph.nodes if 'isVerified' in graph.nodes[n]}\n",
        "Nb_followers = {n: graph.nodes[n]['followers_count'] for n in graph.nodes if 'followers_count' in graph.nodes[n]}\n",
        "Nb_favories = {n: graph.nodes[n]['favourites_count'] for n in graph.nodes if 'favourites_count' in graph.nodes[n]}\n",
        "Nb_emis = {n: graph.nodes[n]['statuses_count'] for n in graph.nodes if 'statuses_count' in graph.nodes[n]}\n",
        "Nb_public = {n: graph.nodes[n]['listed_count'] for n in graph.nodes if 'listed_count' in graph.nodes[n]}\n",
        "\n",
        "Df_Utilisateurs1 = pd.DataFrame({'noeud_Util' : Nom_secret.keys(),\n",
        "                               'Nom perso' : Nom_secret.values(),'Compte_verif' : Compte_verif.values(),\n",
        "                               'Nb_followers' : Nb_followers.values(), 'Nb_favories' : Nb_favories.values(),\n",
        "                               'Nb_emis' : Nb_emis.values(), 'Nb_public' : Nb_public.values() })\n",
        "Df_Utilisateurs2 = pd.DataFrame({'noeud_Util' : Utilisateurs_nom.keys(),\n",
        "                               'Nom Utilisateur' : Utilisateurs_nom.values()})\n",
        "Df_Utilisateur = pd.merge(Df_Utilisateurs1,Df_Utilisateurs2, on ='noeud_Util', how = 'inner')\n",
        "Df_Utilisateur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "402dd63c",
      "metadata": {
        "scrolled": true,
        "id": "402dd63c"
      },
      "outputs": [],
      "source": [
        "#On recupere les categories\n",
        "postcategory_nodes = {n : graph.nodes[n][\"id\"] for n in graph.nodes() if graph.nodes[n]['labels'] == \":PostCategory\"}\n",
        "Categori_df = pd.DataFrame({'Evenement' : postcategory_nodes.keys(), 'PostCategory' : postcategory_nodes.values()} )\n",
        "Categori_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c164e49",
      "metadata": {
        "scrolled": true,
        "id": "7c164e49"
      },
      "outputs": [],
      "source": [
        "#Liaison des utilisateurs au tweet\n",
        "\n",
        "liasion_utilisateur = [(source,target) for (source,target) in graph.edges() if source in Nom_secret.keys() and target in tweets.keys() ]\n",
        "liasion_utilisateur_df = pd.DataFrame(liasion_utilisateur)\n",
        "liasion_utilisateur_df = liasion_utilisateur_df.rename(columns= {0 : 'noeud_Util', 1 : \"noeud_tweet\"})\n",
        "liasion_utilisateur_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5548af9",
      "metadata": {
        "id": "f5548af9"
      },
      "outputs": [],
      "source": [
        "#Liaison des categories au post\n",
        "liaison = [(source,target) for (source,target) in graph.edges() if target in postcategory_nodes.keys()]\n",
        "Liason_df = pd.DataFrame(liaison)\n",
        "Liason_df = Liason_df.rename(columns= {0 : 'noeud_tweet', 1 : \"Evenement\"})\n",
        "Liason_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a509e6e1",
      "metadata": {
        "scrolled": false,
        "id": "a509e6e1"
      },
      "outputs": [],
      "source": [
        "#Fusion de tous les dataframe\n",
        "Df_final = pd.merge(Liason_df, Categori_df, on = \"Evenement\", how = 'left')\n",
        "Df_final = pd.merge(Df_final, Tweet_df, on = \"noeud_tweet\")\n",
        "Df_info = pd.merge(liasion_utilisateur_df, Df_Utilisateur, on = 'noeud_Util', how = 'inner')\n",
        "Df_final = pd.merge(Df_info, Df_final, on = 'noeud_tweet', how = 'inner')\n",
        "Df_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e08c1408",
      "metadata": {
        "id": "e08c1408"
      },
      "outputs": [],
      "source": [
        "#On a 36607 tweets differents dans la base\n",
        "len(np.unique(Df_final['noeud_tweet']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bed095ef",
      "metadata": {
        "id": "bed095ef"
      },
      "outputs": [],
      "source": [
        "Category = list(np.unique(Df_final[['PostCategory']]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2364c7ea",
      "metadata": {
        "id": "2364c7ea"
      },
      "outputs": [],
      "source": [
        "#We create a variable for each label\n",
        "\n",
        "for i in Category:\n",
        "    Df_final[i] = np.nan\n",
        "    for j in range(0,len(Df_final)):\n",
        "        if Df_final.loc[j,'PostCategory'] == i :\n",
        "            Df_final.loc[j,i] = 1\n",
        "        else :\n",
        "            Df_final.loc[j,i] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44862262",
      "metadata": {
        "id": "44862262"
      },
      "outputs": [],
      "source": [
        "#We encode the variable Niveau, Compte_verif and Sensitivite\n",
        "Df_final['Compte_verif'] = Df_final['Compte_verif'].replace([True,False], [0,1])\n",
        "Df_final['Sensitivité'] = Df_final['Sensitivité'].replace([True,False], [0,1])\n",
        "Df_final['Niveau'] = Df_final['Niveau'].replace(['Unknown', 'Low', 'Medium','High','Critical'], [0,1, 2,3,4])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e73602f4",
      "metadata": {
        "id": "e73602f4"
      },
      "outputs": [],
      "source": [
        "#WordCloud for the tweets of News\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "lis = ''\n",
        "\n",
        "for tweet in Df_final['Contenu'][Df_final[\"PostCategory\"]== \"News\"] :\n",
        "    lis += tweet\n",
        "\n",
        "wordcloud = WordCloud(width = 800, height = 800, min_font_size = 10).generate(lis)\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.title(\"WordCloud for News\")\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad25a995",
      "metadata": {
        "id": "ad25a995"
      },
      "outputs": [],
      "source": [
        "#We aggregate per tweet\n",
        "\n",
        "Df_final = Df_final.groupby(['noeud_tweet','Nom perso', 'Compte_verif', 'Nb_followers', 'Nb_favories', 'Nb_emis', 'Nb_public', 'Nom Utilisateur', 'Contenu', 'Niveau', 'Nb_retweet', 'Sensitivité'], as_index=False).sum()\n",
        "Df_final = Df_final.drop(['noeud_Util','Evenement', 'PostCategory'], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "297ec373",
      "metadata": {
        "id": "297ec373"
      },
      "outputs": [],
      "source": [
        "#Study of the proportion of each label\n",
        "Proportion = list()\n",
        "\n",
        "for i in Category :\n",
        "    Proportion.append(Df_final[i].sum()*100/len(Df_final))\n",
        "Repartition_df = pd.DataFrame({'Category' : Category, 'Freq' : Proportion })\n",
        "Repartition_df = Repartition_df.sort_values('Freq')\n",
        "#We create downgrade color for the barplot\n",
        "color_map = plt.cm.get_cmap('Reds')\n",
        "colors = color_map(np.linspace(0, 1, len(Proportion)))\n",
        "#Barplot of proportion of each classes.\n",
        "plt.barh(Repartition_df['Category'], Repartition_df['Freq'], color = colors)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.xlabel('Frequence')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c0886bb",
      "metadata": {
        "id": "0c0886bb"
      },
      "source": [
        "# SPLIT TRAIN TEST DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da389dea",
      "metadata": {
        "id": "da389dea"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "y = np.asarray(Df_final[Df_final.columns[13:38]])\n",
        "#X = Df_final.drop(Category, axis = 1)\n",
        "X = Df_final\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=43)\n",
        "X_train.reset_index(drop=True, inplace = True)\n",
        "X_test.reset_index(drop=True, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02a2607d",
      "metadata": {
        "id": "02a2607d"
      },
      "outputs": [],
      "source": [
        "len(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96a81d37",
      "metadata": {
        "id": "96a81d37"
      },
      "outputs": [],
      "source": [
        "len(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a41f88c7",
      "metadata": {
        "id": "a41f88c7"
      },
      "outputs": [],
      "source": [
        "#Proportion in the train\n",
        "for i in Category :\n",
        "    print('Catgory', i, X_train[i].sum()*100/len(X_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a0cc87e",
      "metadata": {
        "id": "9a0cc87e"
      },
      "outputs": [],
      "source": [
        "#Proportion in the test\n",
        "for i in Category :\n",
        "    print('Catgory', i, X_test[i].sum()*100/len(X_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "748797e3",
      "metadata": {
        "id": "748797e3"
      },
      "source": [
        "# Reechantillonage : Up et Down Sampling (TRAIN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23f13701",
      "metadata": {
        "id": "23f13701"
      },
      "outputs": [],
      "source": [
        "# UpSampling\n",
        "Propmin = 10\n",
        "index_CleanUp = X_train[X_train['CleanUp'] == 1]\n",
        "index_NewSubEvent = X_train[X_train['NewSubEvent'] == 1]\n",
        "index_Volunteer = X_train[X_train['Volunteer'] == 1]\n",
        "index_GoodsServices = X_train[X_train['GoodsServices'] == 1]\n",
        "index_SearchAndRescue = X_train[X_train['SearchAndRescue'] == 1]\n",
        "index_InformationWanted = X_train[X_train['InformationWanted'] == 1]\n",
        "index_Donations = X_train[X_train['Donations'] == 1]\n",
        "index_MovePeople = X_train[X_train['MovePeople'] == 1]\n",
        "\n",
        "AM1 = index_CleanUp.sample(round(((Propmin - 0.6) / 100 * len(X_train))), random_state=42, replace=True).index\n",
        "AM2 = index_NewSubEvent.sample(round((Propmin - 2.5) / 100 * len(X_train)), random_state=42, replace=True).index\n",
        "AM4 = index_Volunteer.sample(round(((Propmin - 0.57) / 100 * len(X_train))), random_state=42, replace=True).index\n",
        "AM5 = index_GoodsServices.sample(round((Propmin - 0.50) / 100 * len(X_train)), random_state=42, replace=True).index\n",
        "AM8 = index_SearchAndRescue.sample(round((Propmin - 0.71) / 100 * len(X_train)), random_state=42, replace=True).index\n",
        "AM9 = index_InformationWanted.sample(round((Propmin - 0.76) / 100 * len(X_train)), random_state=42, replace=True).index\n",
        "AM11 = index_Donations.sample(round(((Propmin - 2.6) / 100 * len(X_train))), random_state=42, replace=True).index\n",
        "AM13 = index_MovePeople.sample(round((Propmin - 1) / 100 * len(X_train)), random_state=42, replace=True).index\n",
        "\n",
        "AMETTRE = list((list(AM1) + list(AM2) + list(AM4) + list(AM5) + list(AM8) + list(AM9) + list(AM11) + list(AM13)))\n",
        "X_train_resample = X_train.loc[AMETTRE]\n",
        "X_train_resample = pd.concat([X_train, X_train_resample])\n",
        "\n",
        "# Resampling pour rebalancer les classes : Downsampling\n",
        "Propmax = 20\n",
        "index_Irrelevant = X_train[X_train['Irrelevant'] == 1]\n",
        "index_ThirdPartyObservation = X_train[X_train['ThirdPartyObservation'] == 1]\n",
        "index_FirstPartyObservation = X_train[X_train['FirstPartyObservation'] == 1]\n",
        "index_Hashtags = X_train[X_train['Hashtags'] == 1]\n",
        "index_Location = X_train[X_train['Location'] == 1]\n",
        "index_MultimediaShare = X_train[X_train['MultimediaShare'] == 1]\n",
        "index_Sentiment = X_train[X_train['Sentiment'] == 1]\n",
        "index_News = X_train[X_train['News'] == 1]\n",
        "\n",
        "AR2 = index_Sentiment.sample(round((23 - Propmax) / 100 * len(X_train_resample)), random_state=42, replace=True).index\n",
        "AR4 = index_MultimediaShare.sample(round((26 - Propmax) / 100 * len(X_train_resample)), random_state=42, replace=True).index\n",
        "AR5 = index_Location.sample(round((22 - Propmax) / 100 * len(X_train_resample)), random_state=42, replace=True).index\n",
        "AR7 = index_Hashtags.sample(round((31 - Propmax) / 100 * len(X_train_resample)), random_state=42, replace=True).index\n",
        "AR8 = index_News.sample(round((29 - Propmax) / 100 * len(X_train_resample)), random_state=42, replace=True).index\n",
        "ASUPP = list(set(list(AR2) + list(AR4) + list(AR2) + list(AR5) + list(AR7) + list(AR8)))\n",
        "X_train_resample = X_train_resample.drop(ASUPP)\n",
        "\n",
        "X_train = X_train_resample\n",
        "X_train.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2d5f11a",
      "metadata": {
        "id": "b2d5f11a"
      },
      "outputs": [],
      "source": [
        "#Study of the proportion of each label\n",
        "Proportion = list()\n",
        "\n",
        "for i in Category :\n",
        "    Proportion.append(X_train[i].sum()*100/len(X_train))\n",
        "Repartition_df = pd.DataFrame({'Category' : Category, 'Freq' : Proportion })\n",
        "Repartition_df = Repartition_df.sort_values('Freq')\n",
        "#We create downgrade color for the barplot\n",
        "color_map = plt.cm.get_cmap('Reds')\n",
        "colors = color_map(np.linspace(0, 1, len(Proportion)))\n",
        "#Barplot of proportion of each classes.\n",
        "plt.barh(Repartition_df['Category'], Repartition_df['Freq'], color = colors)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.xlabel('Frequence for the train set')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62239c1b",
      "metadata": {
        "id": "62239c1b"
      },
      "source": [
        "# Reechantillonage : Up et Down Sampling (TEST)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b21a8e1",
      "metadata": {
        "id": "4b21a8e1"
      },
      "outputs": [],
      "source": [
        "# UpSampling\n",
        "Propmin = 10\n",
        "index_CleanUp = X_test[X_test['CleanUp'] == 1]\n",
        "index_NewSubEvent = X_test[X_test['NewSubEvent'] == 1]\n",
        "index_Volunteer = X_test[X_test['Volunteer'] == 1]\n",
        "index_GoodsServices = X_test[X_test['GoodsServices'] == 1]\n",
        "index_SearchAndRescue = X_test[X_test['SearchAndRescue'] == 1]\n",
        "index_InformationWanted = X_test[X_test['InformationWanted'] == 1]\n",
        "index_Donations = X_test[X_test['Donations'] == 1]\n",
        "index_MovePeople = X_test[X_test['MovePeople'] == 1]\n",
        "\n",
        "AM1 = index_CleanUp.sample(round(((Propmin - 0.6) / 100 * len(X_test))), random_state=42, replace=True).index\n",
        "AM2 = index_NewSubEvent.sample(round((Propmin - 2.5) / 100 * len(X_test)), random_state=42, replace=True).index\n",
        "AM4 = index_Volunteer.sample(round(((Propmin - 0.57) / 100 * len(X_test))), random_state=42, replace=True).index\n",
        "AM5 = index_GoodsServices.sample(round((Propmin - 0.50) / 100 * len(X_test)), random_state=42, replace=True).index\n",
        "AM8 = index_SearchAndRescue.sample(round((Propmin - 0.71) / 100 * len(X_test)), random_state=42, replace=True).index\n",
        "AM9 = index_InformationWanted.sample(round((Propmin - 0.76) / 100 * len(X_test)), random_state=42, replace=True).index\n",
        "AM11 = index_Donations.sample(round(((Propmin - 2.6) / 100 * len(X_test))), random_state=42, replace=True).index\n",
        "AM13 = index_MovePeople.sample(round((Propmin - 1) / 100 * len(X_test)), random_state=42, replace=True).index\n",
        "\n",
        "AMETTRE = list((list(AM1) + list(AM2) + list(AM4) + list(AM5) + list(AM8) + list(AM9) + list(AM11) + list(AM13)))\n",
        "X_test_resample = X_test.loc[AMETTRE]\n",
        "X_test_resample = pd.concat([X_test, X_test_resample])\n",
        "\n",
        "# Resampling pour rebalancer les classes : Downsampling\n",
        "Propmax = 20\n",
        "index_Irrelevant = X_test[X_test['Irrelevant'] == 1]\n",
        "index_ThirdPartyObservation = X_test[X_test['ThirdPartyObservation'] == 1]\n",
        "index_FirstPartyObservation = X_test[X_test['FirstPartyObservation'] == 1]\n",
        "index_Hashtags = X_test[X_test['Hashtags'] == 1]\n",
        "index_Location = X_test[X_test['Location'] == 1]\n",
        "index_MultimediaShare = X_test[X_test['MultimediaShare'] == 1]\n",
        "index_Sentiment = X_test[X_test['Sentiment'] == 1]\n",
        "index_News = X_test[X_test['News'] == 1]\n",
        "\n",
        "AR2 = index_Sentiment.sample(round((23 - Propmax) / 100 * len(X_test_resample)), random_state=42, replace=True).index\n",
        "AR4 = index_MultimediaShare.sample(round((26 - Propmax) / 100 * len(X_test_resample)), random_state=42, replace=True).index\n",
        "AR5 = index_Location.sample(round((22 - Propmax) / 100 * len(X_test_resample)), random_state=42, replace=True).index\n",
        "AR7 = index_Hashtags.sample(round((31 - Propmax) / 100 * len(X_test_resample)), random_state=42, replace=True).index\n",
        "AR8 = index_News.sample(round((29 - Propmax) / 100 * len(X_test_resample)), random_state=42, replace=True).index\n",
        "ASUPP = list(set(list(AR2) + list(AR4) + list(AR2) + list(AR5) + list(AR7) + list(AR8)))\n",
        "X_test_resample = X_test_resample.drop(ASUPP)\n",
        "\n",
        "X_test = X_test_resample\n",
        "X_test.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "940f1e31",
      "metadata": {
        "id": "940f1e31"
      },
      "outputs": [],
      "source": [
        "Proportion_resample = list()\n",
        "\n",
        "for i in Category :\n",
        "    Proportion_resample.append(X_test[i].sum()*100/len(X_test))\n",
        "\n",
        "Repartition_df_resample = pd.DataFrame({'Category' : Category, 'Freq' : Proportion_resample })\n",
        "Repartition_df_resample = Repartition_df_resample.sort_values('Freq')\n",
        "#We create downgrade color for the barplot\n",
        "color_map = plt.cm.get_cmap('Reds')\n",
        "colors = color_map(np.linspace(0, 1, len(Proportion_resample)))\n",
        "#Barplot of proportion of each classes.\n",
        "plt.barh(Repartition_df_resample['Category'], Repartition_df_resample['Freq'], color = colors)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.xlabel('Frequence for the test set')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7546950",
      "metadata": {
        "id": "d7546950"
      },
      "outputs": [],
      "source": [
        "len(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13ec5f7d",
      "metadata": {
        "id": "13ec5f7d"
      },
      "outputs": [],
      "source": [
        "len(np.unique(X_train['noeud_tweet']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32fb8d17",
      "metadata": {
        "id": "32fb8d17"
      },
      "outputs": [],
      "source": [
        "len(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64ef3220",
      "metadata": {
        "id": "64ef3220"
      },
      "outputs": [],
      "source": [
        "len(np.unique(X_test['noeud_tweet']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb140d64",
      "metadata": {
        "id": "bb140d64"
      },
      "outputs": [],
      "source": [
        "#Reattribute the label to y\n",
        "y_train = np.asarray(X_train[X_train.columns[13:38]])\n",
        "y_test = np.asarray(X_test[X_test.columns[13:38]])\n",
        "X_train = X_train.drop(Category, axis = 1)\n",
        "X_test = X_test.drop(Category, axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5622e5b8",
      "metadata": {
        "id": "5622e5b8"
      },
      "source": [
        "# Create new features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1162ef7",
      "metadata": {
        "scrolled": true,
        "id": "a1162ef7"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Count the number of emoticon\n",
        "def count_emoticons(text):\n",
        "    pattern = r'[:;=x][-]?[)D(opd]'\n",
        "    matches = re.findall(pattern, text)\n",
        "    return len(matches)\n",
        "\n",
        "#If there is a link\n",
        "def link(data, col):\n",
        "    data['link'] = data[col].str.contains('http')\n",
        "    data['link'] =data['link'].astype(int)\n",
        "\n",
        "\n",
        "#Count the number of #\n",
        "def count_hash(text):\n",
        "    pattern = r'[#]'\n",
        "    matches = re.findall(pattern, text)\n",
        "    return len(matches)\n",
        "\n",
        "#Count the nb of number\n",
        "def count_nb(text):\n",
        "    pattern = r'[1-9]'\n",
        "    matches = re.findall(pattern, text)\n",
        "    return len(matches)\n",
        "\n",
        "#Tell if there is a ponctuation ! or ?\n",
        "def ponctuation_nb(text):\n",
        "    pattern = r'[!?]'\n",
        "    matches = re.findall(pattern, text)\n",
        "    return len(matches)\n",
        "\n",
        "\n",
        "#Apply the different function created on our different dataset.\n",
        "X_train['emoticon_count'] = X_train['Contenu'].apply(count_emoticons)\n",
        "X_train['hashtags_count'] = X_train['Contenu'].apply(count_hash)\n",
        "X_train['nb_count'] = X_train['Contenu'].apply(count_nb)\n",
        "X_train['Ponctuation'] = X_train['Contenu'].apply(ponctuation_nb)\n",
        "X_test['emoticon_count'] = X_test['Contenu'].apply(count_emoticons)\n",
        "X_test['hashtags_count'] = X_test['Contenu'].apply(count_hash)\n",
        "X_test['nb_count'] = X_test['Contenu'].apply(count_nb)\n",
        "X_test['Ponctuation'] = X_test['Contenu'].apply(ponctuation_nb)\n",
        "link(X_train,'Contenu')\n",
        "link(X_test,'Contenu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "612ee37d",
      "metadata": {
        "id": "612ee37d"
      },
      "outputs": [],
      "source": [
        "#Store the list of stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "X_train['Clean_text'] = \"\"\n",
        "# We drop the link, the stop words\n",
        "for index, sentence in enumerate(X_train['Contenu']):\n",
        "    sentence = re.sub(r'http\\S+', \" \", sentence)\n",
        "    sentence = re.sub(r'[@#-]\\S+', \" \", sentence)\n",
        "    sentence = re.sub(r'RT', \" \", sentence)\n",
        "    sentence = re.sub(r\"\\'s\", \" \", sentence)\n",
        "    sentence = re.sub(r\"\\'m\", \" am \", sentence)\n",
        "    sentence = re.sub(r\"\\'ve\", \" have \", sentence)\n",
        "    sentence = re.sub(r\"n't\", \" not \", sentence)\n",
        "    sentence = re.sub(r\"\\'re\", \" are \", sentence)\n",
        "    sentence = re.sub(r\"\\'ll\", \" will \", sentence)\n",
        "    sentence = re.sub(r\"\\'d\", \" would \", sentence)\n",
        "    sentence = re.sub(r\"can't\", \"can not \", sentence)\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sentence) #Caracter without letters\n",
        "    # Tokenization\n",
        "    words = word_tokenize(sentence)\n",
        "    words = [word.lower() for word in words]\n",
        "    # We drop the stop words and the punctuation\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words and word.lower() not in string.punctuation]\n",
        "    filtered_sentence = ' '.join(filtered_words)\n",
        "    X_train.loc[index,'Clean_text'] = filtered_sentence\n",
        "\n",
        "X_test['Clean_text'] = \"\"\n",
        "# We drop the link, the stop words\n",
        "for index, sentence in enumerate(X_test['Contenu']):\n",
        "    sentence = re.sub(r'http\\S+', \" \", sentence)\n",
        "    sentence = re.sub(r'[@#-]\\S+', \" \", sentence)\n",
        "    sentence = re.sub(r'RT', \" \", sentence)\n",
        "    sentence = re.sub(r\"\\'s\", \" \", sentence)\n",
        "    sentence = re.sub(r\"\\'m\", \" am \", sentence)\n",
        "    sentence = re.sub(r\"\\'ve\", \" have \", sentence)\n",
        "    sentence = re.sub(r\"n't\", \" not \", sentence)\n",
        "    sentence = re.sub(r\"\\'re\", \" are \", sentence)\n",
        "    sentence = re.sub(r\"\\'ll\", \" will \", sentence)\n",
        "    sentence = re.sub(r\"\\'d\", \" would \", sentence)\n",
        "    sentence = re.sub(r\"can't\", \"can not \", sentence)\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "    # Tokenization\n",
        "    words = word_tokenize(sentence)\n",
        "    words = [word.lower() for word in words]\n",
        "    # We drop the stop words and the punctuation\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words and word.lower() not in string.punctuation]\n",
        "    filtered_sentence = ' '.join(filtered_words)\n",
        "    X_test.loc[index,'Clean_text'] = filtered_sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2931b8cf",
      "metadata": {
        "id": "2931b8cf"
      },
      "outputs": [],
      "source": [
        "# X_train['Clean_Nom'] = \"\"\n",
        "# for index, sentence in enumerate(X_train['Nom Utilisateur']):\n",
        "#     sentence = re.sub(r'http\\S+', \" \", sentence)\n",
        "#     # Tokenization des mots dans la phrase\n",
        "#     words = word_tokenize(sentence)\n",
        "#     # Filtrage des mots qui ne sont pas des stopwords\n",
        "#     filtered_words = [word for word in words if word.lower() not in stop_words and word.lower() not in string.punctuation]\n",
        "#     # Reconstitution de la phrase sans les stopwords\n",
        "#     filtered_sentence = ' '.join(filtered_words)\n",
        "#     # Ajout de la phrase filtrée à la nouvelle colonne de texte\n",
        "#     X_train.loc[index,'Clean_Nom'] = filtered_sentence\n",
        "\n",
        "# X_test['Clean_Nom'] = \"\"\n",
        "# for index, sentence in enumerate(X_test['Nom Utilisateur']):\n",
        "#     sentence = re.sub(r'http\\S+', \" \", sentence)\n",
        "#     # Tokenization des mots dans la phrase\n",
        "#     words = word_tokenize(sentence)\n",
        "#     # Filtrage des mots qui ne sont pas des stopwords\n",
        "#     filtered_words = [word for word in words if word.lower() not in stop_words and word.lower() not in string.punctuation]\n",
        "#     # Reconstitution de la phrase sans les stopwords\n",
        "#     filtered_sentence = ' '.join(filtered_words)\n",
        "#     # Ajout de la phrase filtrée à la nouvelle colonne de texte\n",
        "#     X_test.loc[index,'Clean_Nom'] = filtered_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6da9bbc",
      "metadata": {
        "id": "d6da9bbc"
      },
      "outputs": [],
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "#Negativ polarity of the tweet\n",
        "def neg_sentiment(text):\n",
        "    sentiment_score = sia.polarity_scores(text)\n",
        "    return sentiment_score['neg']\n",
        "#Positive polarity of the tweet\n",
        "def pos_sentiment(text):\n",
        "    sentiment_score = sia.polarity_scores(text)\n",
        "    return sentiment_score['pos']\n",
        "#Neutral polarity of the tweet\n",
        "def neutral_sentiment(text):\n",
        "    sentiment_score = sia.polarity_scores(text)\n",
        "    return sentiment_score['neu']\n",
        "X_train['Positive polarity'] = X_train['Contenu'].apply(pos_sentiment)\n",
        "X_train['Neutral polarity'] = X_train['Contenu'].apply(neutral_sentiment)\n",
        "X_train['Negative_polarity'] = X_train['Contenu'].apply(neg_sentiment)\n",
        "X_test['Positive polarity'] = X_test['Contenu'].apply(pos_sentiment)\n",
        "X_test['Neutral polarity'] = X_test['Contenu'].apply(neutral_sentiment)\n",
        "X_test['Negative_polarity'] = X_test['Contenu'].apply(neg_sentiment)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e492b383",
      "metadata": {
        "scrolled": true,
        "id": "e492b383"
      },
      "outputs": [],
      "source": [
        "#We compute the length of clean text\n",
        "X_train['longueur'] = X_train['Clean_text'].apply(len)\n",
        "X_test['longueur'] = X_test['Clean_text'].apply(len)\n",
        "X_train"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "136ef9e2",
      "metadata": {
        "id": "136ef9e2"
      },
      "source": [
        "# OPTION 1 : SENTENCE EMBEDDING"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f420dfb9",
      "metadata": {
        "id": "f420dfb9"
      },
      "source": [
        "This option for make word embedding perform well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9bc7c35",
      "metadata": {
        "scrolled": true,
        "id": "f9bc7c35"
      },
      "outputs": [],
      "source": [
        "\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "# model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# # for index, sentence in enumerate(Df_final['Clean_text']):\n",
        "# #     Df_final.loc[index,'mean_embedding'] = list(model.encode(sentence))\n",
        "\n",
        "# X_train['mean'] = X_train['Clean_text'].apply(lambda x : list(model.encode(x)))\n",
        "\n",
        "# X_test['mean'] = X_test['Clean_text'].apply(lambda x : list(model.encode(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaaa5e34",
      "metadata": {
        "id": "eaaa5e34"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "X_train['mean'] = X_train['Clean_text'].apply(lambda x : list(sbert_model.encode(x)))\n",
        "\n",
        "X_test['mean'] = X_test['Clean_text'].apply(lambda x : list(sbert_model.encode(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3056acc",
      "metadata": {
        "scrolled": true,
        "id": "d3056acc"
      },
      "outputs": [],
      "source": [
        "def extract_elements(row):\n",
        "    return pd.Series(row['mean'])\n",
        "\n",
        "# Appliquer la fonction de transformation sur la colonne et créer les nouvelles colonnes\n",
        "new_columns = X_train.apply(extract_elements, axis=1)\n",
        "\n",
        "# Ajouter les nouvelles colonnes au DataFrame original\n",
        "X_train = pd.concat([X_train, new_columns], axis=1)\n",
        "\n",
        "# Renommer les nouvelles colonnes si nécessaire\n",
        "X_train.rename(columns={i: f'Embedding_tweet_sentence{i+1}' for i in range(len(new_columns.columns))}, inplace=True)\n",
        "\n",
        "new_columns = X_test.apply(extract_elements, axis=1)\n",
        "\n",
        "# Ajouter les nouvelles colonnes au DataFrame original\n",
        "X_test = pd.concat([X_test, new_columns], axis=1)\n",
        "\n",
        "# Renommer les nouvelles colonnes si nécessaire\n",
        "X_test.rename(columns={i: f'Embedding_tweet_sentence{i+1}' for i in range(len(new_columns.columns))}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "471fcc1c",
      "metadata": {
        "id": "471fcc1c"
      },
      "outputs": [],
      "source": [
        "\n",
        "X_train =X_trainbis\n",
        "X_test = X_testbis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dcd51a2",
      "metadata": {
        "id": "8dcd51a2"
      },
      "source": [
        "# WORD EMBEDDING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b742a49",
      "metadata": {
        "id": "6b742a49"
      },
      "outputs": [],
      "source": [
        "# sentences = [word_tokenize(sentence) for sentence in Df_final['Clean_text']]\n",
        "# model = Word2Vec(sentences=sentences, vector_size=50, window=5, min_count=1, workers=4)\n",
        "# Df_final['embeddings_Contenu'] = Df_final['Clean_text'].apply(lambda x: [model.wv[word] for word in x if word in model.wv])\n",
        "# vocabulary = list(model.wv.index_to_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67cf587a",
      "metadata": {
        "id": "67cf587a"
      },
      "outputs": [],
      "source": [
        "# sentences_nom = [word_tokenize(sentence) for sentence in X_train['Nom Utilisateur']]\n",
        "# model_nom = Word2Vec(sentences=sentences_nom, vector_size=50, window=5, min_count=1, workers=4)\n",
        "# X_train['embeddings_Contenu_Nom'] = X_train['Clean_Nom'].apply(lambda x: [model_nom.wv[word] for word in x if word in model_nom.wv])\n",
        "# list(model_nom.wv.index_to_key)\n",
        "\n",
        "# sentences_nom = [word_tokenize(sentence) for sentence in X_test['Nom Utilisateur']]\n",
        "# model_nom = Word2Vec(sentences=sentences_nom, vector_size=50, window=5, min_count=1, workers=4)\n",
        "# X_test['embeddings_Contenu_Nom'] = X_test['Clean_Nom'].apply(lambda x: [model_nom.wv[word] for word in x if word in model_nom.wv])\n",
        "# list(model_nom.wv.index_to_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4e304d7",
      "metadata": {
        "id": "a4e304d7"
      },
      "outputs": [],
      "source": [
        "# def calculate_mean_embedding(embeddings):\n",
        "#     if embeddings:\n",
        "#         return np.sum(embeddings, axis=0)\n",
        "#     else:\n",
        "#         # Si aucun mot n'a d'embedding, retourner un vecteur de zéros\n",
        "#         return np.zeros(50)  # Assurez-vous que la taille du vecteur correspond à la dimension de vos embeddings\n",
        "\n",
        "# # Ajouter une nouvelle colonne avec les moyennes des embeddings\n",
        "# #Df_final['mean_embedding'] = Df_final['embeddings_Contenu'].apply(calculate_mean_embedding)\n",
        "# X_train['mean_embedding_Nom'] = X_train['embeddings_Contenu_Nom'].apply(calculate_mean_embedding)\n",
        "# X_test['mean_embedding_Nom'] = X_test['embeddings_Contenu_Nom'].apply(calculate_mean_embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a83f0ce5",
      "metadata": {
        "id": "a83f0ce5"
      },
      "outputs": [],
      "source": [
        "# def extract_elements(row):\n",
        "#     return pd.Series(row['mean_embedding'])\n",
        "\n",
        "# # Appliquer la fonction de transformation sur la colonne et créer les nouvelles colonnes\n",
        "# new_columns = Df_final.apply(extract_elements, axis=1)\n",
        "\n",
        "# # Ajouter les nouvelles colonnes au DataFrame original\n",
        "# Df_final = pd.concat([Df_final, new_columns], axis=1)\n",
        "\n",
        "# # Renommer les nouvelles colonnes si nécessaire\n",
        "# Df_final.rename(columns={i: f'Embedding_tweet{i+1}' for i in range(len(new_columns.columns))}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4a8843e",
      "metadata": {
        "id": "a4a8843e"
      },
      "outputs": [],
      "source": [
        "# def extract_elements_Nom(row):\n",
        "#     return pd.Series(row['mean_embedding_Nom'])\n",
        "\n",
        "# # Appliquer la fonction de transformation sur la colonne et créer les nouvelles colonnes\n",
        "# new_columns = X_train.apply(extract_elements_Nom, axis=1)\n",
        "\n",
        "# # Ajouter les nouvelles colonnes au DataFrame original\n",
        "# X_train = pd.concat([X_train, new_columns], axis=1)\n",
        "\n",
        "# # Renommer les nouvelles colonnes si nécessaire\n",
        "# X_train.rename(columns={i: f'Embedding_Nom{i+1}' for i in range(len(new_columns.columns))}, inplace=True)\n",
        "\n",
        "# # Appliquer la fonction de transformation sur la colonne et créer les nouvelles colonnes\n",
        "# new_columns = X_test.apply(extract_elements_Nom, axis=1)\n",
        "\n",
        "# # Ajouter les nouvelles colonnes au DataFrame original\n",
        "# X_test = pd.concat([X_test, new_columns], axis=1)\n",
        "\n",
        "# # Renommer les nouvelles colonnes si nécessaire\n",
        "# X_test.rename(columns={i: f'Embedding_Nom{i+1}' for i in range(len(new_columns.columns))}, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a59be2b8",
      "metadata": {
        "id": "a59be2b8"
      },
      "source": [
        "# Node embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca7a0c52",
      "metadata": {
        "id": "ca7a0c52"
      },
      "source": [
        "We have to drop edges between tweet and postcategory wether the model will be biais."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77702913",
      "metadata": {
        "id": "77702913"
      },
      "outputs": [],
      "source": [
        "#We remove nodes which are Category\n",
        "nodes_to_remove = []\n",
        "\n",
        "\n",
        "for node, attributes in graph.nodes(data=True):\n",
        "    if attributes.get('labels') == ':PostCategory':\n",
        "        nodes_to_remove.append(node)\n",
        "\n",
        "for node in nodes_to_remove:\n",
        "    graph.remove_node(node)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ba7b087",
      "metadata": {
        "id": "4ba7b087"
      },
      "outputs": [],
      "source": [
        "p = 1 # Paramètre de probabilité de retour arrière\n",
        "q = 0.7  # Paramètre de contrôle de l'exploration\n",
        "dimensions = 100  # Taille de l'embedding\n",
        "num_walks = 80  # Nombre de promenades aléatoires par nœud\n",
        "walk_length = 90  # Longueur des promenades aléatoires\n",
        "tweet_embeddings={}\n",
        "\n",
        "node2vec = Node2Vec(graph, dimensions=dimensions, walk_length=walk_length, num_walks=num_walks, p=p, q=q)\n",
        "model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
        "\n",
        "for node, attributes in graph.nodes(data=True):\n",
        "    if attributes.get('labels') == ':Tweet':\n",
        "        tweet_embeddings[node] = model.wv[node]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af9e09f9",
      "metadata": {
        "scrolled": true,
        "id": "af9e09f9"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 100\n",
        "for i in range(embedding_dim):\n",
        "    X_train[f\"embedding_{i+1}\"] = None\n",
        "    X_test[f\"embedding_{i+1}\"] = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61694ef1",
      "metadata": {
        "scrolled": true,
        "id": "61694ef1"
      },
      "outputs": [],
      "source": [
        "for i, row in X_train.iterrows():\n",
        "    node = row['noeud_tweet']\n",
        "    if node in tweet_embeddings:\n",
        "        embedding = tweet_embeddings[node]\n",
        "        for j, val in enumerate(embedding):\n",
        "            X_train.at[i, f\"embedding_{j+1}\"] = val\n",
        "\n",
        "for i, row in X_test.iterrows():\n",
        "    node = row['noeud_tweet']\n",
        "    if node in tweet_embeddings:\n",
        "        embedding = tweet_embeddings[node]\n",
        "        for j, val in enumerate(embedding):\n",
        "            X_test.at[i, f\"embedding_{j+1}\"] = val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94b6d173",
      "metadata": {
        "id": "94b6d173"
      },
      "outputs": [],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53f9d3bd",
      "metadata": {
        "scrolled": true,
        "id": "53f9d3bd"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.drop(['noeud_tweet', 'Clean_text', 'Contenu', 'Nom perso', 'Nom Utilisateur',  'mean'], axis=1)\n",
        "X_test = X_test.drop(['noeud_tweet', 'Clean_text', 'Contenu', 'Nom perso', 'Nom Utilisateur',  'mean'], axis=1)\n",
        "\n",
        "\n",
        "#Scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Normalize\n",
        "X_traindd = scaler.fit_transform(X_train)\n",
        "X_testdd = scaler.fit_transform(X_test)\n",
        "\n",
        "# Remettre le résultat dans un dataframe avec les mêmes colonnes\n",
        "X_train = pd.DataFrame(X_traindd, columns=X_train.columns)\n",
        "X_test = pd.DataFrame(X_testdd, columns=X_test.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "603bdd26",
      "metadata": {
        "id": "603bdd26"
      },
      "outputs": [],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e3e7a47",
      "metadata": {
        "id": "5e3e7a47"
      },
      "source": [
        "# Multi Label Classification : Logistic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "729dbd26",
      "metadata": {
        "scrolled": true,
        "id": "729dbd26"
      },
      "outputs": [],
      "source": [
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "clf = MultiOutputClassifier(LogisticRegression()).fit(X_train[X_train.columns[:985]], y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f4f0f50",
      "metadata": {
        "id": "3f4f0f50"
      },
      "outputs": [],
      "source": [
        "y_pred = clf.predict(X_test[X_test.columns[:985]])\n",
        "y_pred\n",
        "f1 = f1_score(y_test, y_pred,average='micro')\n",
        "f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "434bd29e",
      "metadata": {
        "id": "434bd29e"
      },
      "outputs": [],
      "source": [
        "df_cat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "978b5b58",
      "metadata": {
        "id": "978b5b58"
      },
      "outputs": [],
      "source": [
        "# Calculez le F1-score par catégorie\n",
        "f1_scores = f1_score(y_test, y_pred, average=None)\n",
        "\n",
        "score =list()\n",
        "# Affichez les F1-scores par catégorie\n",
        "for i, label in enumerate(Category):\n",
        "    score.append(f1_scores[i])\n",
        "    print(f\"F1-score for category {label}: {f1_scores[i]}\")\n",
        "\n",
        "df_cat = pd.DataFrame({'Cat' : Category, 'F1' : score}, index = range(1,26))\n",
        "df_cat =df_cat.sort_values('F1')\n",
        "plt.barh(df_cat['Cat'], df_cat['F1'], color =colors)\n",
        "plt.xlabel('F1 Score Logistic')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03d54fb4",
      "metadata": {
        "id": "03d54fb4"
      },
      "source": [
        "# MULTI LABEL : RANDOM FOREST CLASSIFIER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "528f6e85",
      "metadata": {
        "id": "528f6e85"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model = MultiOutputClassifier(RandomForestClassifier(n_estimators = 100))\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the F1-score with micro average\n",
        "f1 = f1_score(y_test, y_pred,average='micro')\n",
        "\n",
        "print(\"The F1-score is equal to:\", f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eeae7af2",
      "metadata": {
        "id": "eeae7af2"
      },
      "source": [
        "# OPTIMISATION BAESIENNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2461ec3",
      "metadata": {
        "id": "b2461ec3"
      },
      "outputs": [],
      "source": [
        "#1ere approche HyperTunning avec Grid Search\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "rf_params = { \"n_estimators\" :[300,400,600],\n",
        "            \"max_depth\" : [8,11,15],\n",
        "            \"min_samples_split\" : [2,5,8]}\n",
        "rf_model = GridSearchCV(model, rf_params, cv =3, n_jobs = -1, verbose = 2).fit(X_train, y_train)\n",
        "rf_model.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0f99fe1",
      "metadata": {
        "id": "a0f99fe1"
      },
      "outputs": [],
      "source": [
        "#Code :https://www.kaggle.com/code/adityarawat0701/xgboost-multilabel-classification-bayesian-opt\n",
        "from bayes_opt import BayesianOptimization\n",
        "def xgb_cv(max_depth, learning_rate, subsample, colsample_bytree,min_child_weight):\n",
        "    params = {'objective': 'binary:logistic',\n",
        "              'max_depth': int(max_depth),\n",
        "              'learning_rate': learning_rate,\n",
        "              'subsample': subsample,\n",
        "              'min_child_weight': min_child_weight,\n",
        "              'colsample_bytree': colsample_bytree}\n",
        "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "    cv_result = xgb.cv(params, dtrain, num_boost_round=100, early_stopping_rounds=10, nfold=5, metrics='error')\n",
        "    return -cv_result['test-error-mean'].iloc[-1]\n",
        "\n",
        "pbounds = {'max_depth': (3, 9),\n",
        "           'learning_rate': (0.01, 0.5),\n",
        "           'subsample': (0.1, 1),\n",
        "           'colsample_bytree': (0.1, 1),\n",
        "           'min_child_weight': (1, 12)\n",
        "          }\n",
        "\n",
        "print('Performing hyperparameter tuning using Bayesian optimization...')\n",
        "optimizer = BayesianOptimization(f=xgb_cv, pbounds=pbounds, random_state=1)\n",
        "optimizer.maximize(init_points=5, n_iter=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "654053b9",
      "metadata": {
        "id": "654053b9"
      },
      "source": [
        "# XG BOOST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a5bf200",
      "metadata": {
        "id": "3a5bf200"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "xgbb = xgb.XGBClassifier()\n",
        "xgbb.fit(X_train,y_train)\n",
        "\n",
        "y_pred = xgbb.predict(X_test)\n",
        "\n",
        "#Calculate the F1-score\n",
        "f1 = f1_score(y_test, y_pred,average='micro')\n",
        "f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee462f50",
      "metadata": {
        "id": "ee462f50"
      },
      "outputs": [],
      "source": [
        "# Calculez le F1-score par catégorie\n",
        "f1_scores = f1_score(y_test, y_pred, average=None)\n",
        "\n",
        "score =list()\n",
        "# Affichez les F1-scores par catégorie\n",
        "for i, label in enumerate(Category):\n",
        "    score.append(f1_scores[i])\n",
        "    print(f\"F1-score for category {label}: {f1_scores[i]}\")\n",
        "\n",
        "df_cat = pd.DataFrame({'Cat' : Category, 'F1' : score}, index = range(1,26))\n",
        "df_cat =df_cat.sort_values('F1')\n",
        "plt.barh(df_cat['Cat'], df_cat['F1'], color =colors)\n",
        "plt.xlabel('F1 Score XGBoost')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bc1e3dd",
      "metadata": {
        "id": "0bc1e3dd"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "rf_params = {\"learning_rate\": [0.05, 0.1,0.3],\n",
        "            \"max_depth\" : [7,9,5]}\n",
        "rf_model = GridSearchCV(xgbb, rf_params, cv =3, n_jobs = -1, verbose = 2).fit(X_train, y_train)\n",
        "rf_model.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ca1f502",
      "metadata": {
        "id": "4ca1f502"
      },
      "outputs": [],
      "source": [
        "xgbb = xgb.XGBClassifier(n_estimators =300)\n",
        "xgbb.fit(X_train,y_train)\n",
        "\n",
        "y_pred = xgbb.predict(X_test)\n",
        "\n",
        "#Calculate the F1-score\n",
        "f1 = f1_score(y_test, y_pred,average='micro')\n",
        "f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6a3a9ca",
      "metadata": {
        "id": "d6a3a9ca"
      },
      "outputs": [],
      "source": [
        "from xgboost import plot_importance\n",
        "from matplotlib import pyplot\n",
        "plt.figsize(15,15)\n",
        "plot_importance(xgbb)\n",
        "plt.rcParams['ytick.labelsize'] = 0.1\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c4f7b49",
      "metadata": {
        "id": "1c4f7b49"
      },
      "outputs": [],
      "source": [
        "last_20_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39bc9500",
      "metadata": {
        "id": "39bc9500"
      },
      "outputs": [],
      "source": [
        "importance = xgbb.feature_importances_\n",
        "\n",
        "# Get the indices of the last 20 variables\n",
        "last_20_indices = importance.argsort()[-100:]\n",
        "\n",
        "# Get the names of the last 20 variables\n",
        "last_20_features = [f'Feature {i}' for i in last_20_indices]\n",
        "\n",
        "# Plot the importance of the last 20 variables\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(range(len(last_20_indices)), importance[last_20_indices], align='center')\n",
        "plt.yticks(range(len(last_20_indices)), last_20_features, fontsize=12)\n",
        "plt.xlabel('Importance Score')\n",
        "plt.ylabel('Features')\n",
        "plt.title('Importance of Last 20 Variables')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4413f73",
      "metadata": {
        "id": "c4413f73"
      },
      "source": [
        "# DEEP LEARNING : RESEAU DE NEURONNES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa97a76c",
      "metadata": {
        "scrolled": true,
        "id": "aa97a76c"
      },
      "outputs": [],
      "source": [
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils import to_categorical\n",
        "from keras.metrics import F1Score\n",
        "import keras\n",
        "\n",
        "\n",
        "model =Sequential()\n",
        "model.add(Dense(1000, activation='relu', input_dim = X_train.shape[1] ))\n",
        "model.add(Dense(400, activation='relu'))\n",
        "model.add(Dense(250, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(250, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(32, activation ='relu'))\n",
        "model.add(Dense(25,activation ='sigmoid')\n",
        ")\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['binary_accuracy'])\n",
        "\n",
        "\n",
        "# Entraîner le modèle\n",
        "#model.fit(X_train, y_train, epochs=100, batch_size = 128)\n",
        "history = model.fit(X_train, y_train,validation_split=0.34,shuffle=True,epochs=100,batch_size=150,verbose=0)\n",
        "metrics = model.evaluate(X_test, y_test)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9166725a",
      "metadata": {
        "id": "9166725a"
      },
      "outputs": [],
      "source": [
        "# Calculez le F1-score par catégorie\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Binariser les prédictions et les étiquettes réelles (nécessaire pour le calcul du F1-score)\n",
        "y_pred_binary = np.round(y_pred)\n",
        "y_test_binary = y_test\n",
        "\n",
        "f1_scores = f1_score(y_test, y_pred_binary, average=None)\n",
        "\n",
        "score =list()\n",
        "# Affichez les F1-scores par catégorie\n",
        "for i, label in enumerate(Category):\n",
        "    score.append(f1_scores[i])\n",
        "    print(f\"F1-score for category {label}: {f1_scores[i]}\")\n",
        "\n",
        "df_cat = pd.DataFrame({'Cat' : Category, 'F1' : score}, index = range(1,26))\n",
        "df_cat =df_cat.sort_values('F1')\n",
        "plt.barh(df_cat['Cat'], df_cat['F1'], color =colors)\n",
        "plt.xlabel('F1 Score Neural Network')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a05b7468",
      "metadata": {
        "id": "a05b7468"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['binary_accuracy'])\n",
        "plt.plot(history.history['val_binary_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('binary_accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.yticks(fontsize = 12)\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "088b57d3",
      "metadata": {
        "id": "088b57d3"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Binariser les prédictions et les étiquettes réelles (nécessaire pour le calcul du F1-score)\n",
        "y_pred_binary = np.round(y_pred)\n",
        "y_test_binary = y_test\n",
        "\n",
        "# Calculer le F1-score pour chaque classe (étiquette) et calculer la moyenne\n",
        "f1_scores = f1_score(y_test_binary, y_pred_binary, average='micro')\n",
        "\n",
        "print(\"Le score F1 est égal à:\", f1_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba594190",
      "metadata": {
        "id": "ba594190"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "183a62be",
      "metadata": {
        "scrolled": true,
        "id": "183a62be"
      },
      "outputs": [],
      "source": [
        "f1_scores = f1_score(y_test_binary, y_pred_binary, average=None)\n",
        "\n",
        "# Affichez les F1-scores par catégorie\n",
        "for i, label in enumerate(Category):\n",
        "    print(f\"F1-score for category {label}: {f1_scores[i]}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}